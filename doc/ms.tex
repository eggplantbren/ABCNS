\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\newcommand{\params}{\boldsymbol{\theta}}	% The unknown parameters
\newcommand{\data}{\boldsymbol{D}}  % The data
\newcommand{\dobs}{\boldsymbol{D}_{\rm obs}} % The observed value of the data
\newcommand{\rands}{\boldsymbol{u}}
\newcommand{\dx}{d^N\mathbf{\params}} % Volume element in parameter space
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm

\title{Approximate Bayesian Computation with Nested Sampling}
\author{Brendon J. Brewer}

\begin{document}
\maketitle

\section{Introduction}


\section{Bayesian inference}
By the product rule of probability theory, the prior $p(\params)$ and the
conditional prior for the data $p(\data | \params)$ imply a joint prior
for $\params$ and $\data$:
\begin{equation}
\begin{array}{lr}
p(\params, \data) = p(\params)p(\data|\params)
\end{array}
\end{equation}

The prior $p(\params)$ is a probability distribution over $\Theta$, the space
of possible values of the parameters. The conditional prior for the data,
$p(\data | \params)$, is a family of probability distributions over the space
of possible datasets $\mathcal{D}$. Therefore the joint prior is a probability
distribution over $\Theta \times \mathcal{D}$, which describes our prior
uncertainty about the parameters {\it and the data} \citep{caticha}.

When we learn the value of the data, i.e. that the proposition
$\data = \dobs$ is true for some given value of $\dobs$, our knowledge of
$\params$ is updated from the marginal distribution $p(\params)$ (the prior)
to the specific conditional distribution $p(\params | \data=\dobs)$ given by
the product of the prior and the likelihood:

\begin{eqnarray}
p(\params | \data=\dobs) &\propto& p(\params)p(\data=\dobs | \params)\\
&=& p(\params)\left.P(\data | \params)\right|_{\data=\dobs}.
\end{eqnarray}

\section{Approximate Bayesian Computation}
Approximate Bayesian Computation (ABC), also known as likelihood-free inference,
is a set of Monte Carlo techniques for approximating a posterior distribution
without having to evaluate the likelihood function. Instead, the ability to
generate simulated data sets from the sampling distribution is required.
Since a sampling distribution must be specified, it is not the assumption of
a specific likelihood that is avoided (indeed, a sampling distribution and
a data set imply a particular likelihood function), rather that we cannot
evaluate it cheaply as a function of $\params$ and $\data$.

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lll}
\hline
Bayesian computation		&		Statistical mechanics		&		ABC\\
\hline
Parameter space	$\Theta$	&		Phase space	$\Omega$ or configuration space $\Gamma$ 			& (Parameter$\times$Data) space $\Theta \times \mathcal{D}$\\		
Marginal likelihood / evidence	&	Partition function at $T=1$	&\\


\end{tabular}
\caption{\it The relationship between standard Bayesian computation, statistical
mechanics, and ABC. In each case Monte Carlo methods are used to calculate
integrals over a space. In standard Bayesian computation, it is the parameter
space of a model that is usually of interest, whereas in ABC it is the space
of possible parameter values {\it and} data sets.
\label{tab:relation}}
\end{table}


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{joint.pdf}
\caption{\it The joint prior for parameters $\params$ and data $\data$.
\label{fig:joint}}
\end{figure}

The main challenges associated with ABC are:
\begin{enumerate}
\item The choice of summary statistics
\item The choice of distance function
\item How to choose the value of $\epsilon$, or how to change it as a run
progresses
\item How to achieve good mixing.
\end{enumerate}
Challenges 1 and 2 are Bayesian in nature, i.e. they relate to the very
definition of the posterior distribution and are well defined independent of
the fact that we are going to use a Monte Carlo method to compute the
results. On the other hand, challenges 3 and 4 are about the computational implementation.

\section{Parameterising the data space}
In standard MCMC-based ABC, proposal moves are made by first proposing a
change to the parameters (from $\params$ to $\params'$), and then generating
a mock dataset $\data'$ from $p(\data | \params')$. As a Metropolis proposal
for exploring the product space $\Theta \times \mathcal{D}$ this is a very
aggressive choice, since the proposed value of the
dataset $\params'$ is likely to be very different from the previous one.
One of the most elementary pieces of advice we receive when learning the
Metropolis algorithm is to be careful about how ambitious our proposal
distributions are, since tiny and huge proposal moves are both very
inefficient. The performance of MCMC-based ABC can therefore be improved by
alternative choices of proposal distribution.

In some cases, we might be able to find efficient proposal distributions for exploring $\Theta \times \mathcal{D}$ by having insight into the particular
problem we're working on. However, there are also techniques that are
completely general in that they should work on any ABC-type problem.

Consider the process of generating a mock dataset. Whatever else is involved,
this process will involve calls to an underlying random number generator.
Perhaps a function {\tt rand()}, generating variables from a Uniform(0, 1)
distribution, is called $n$ times in the course of generating a dataset.
Usually, the simulation process has the property that, if $\theta'$ is
close to $\theta$, then the same sequence of random numbers returned by
{\tt rand()} would produce a dataset $\data'$ that is very similar to $\data$.
This suggests an alternative parameterisation of
the product space $\Theta\times\mathcal{D}$: instead of using the one that
naturally describes the data, we can use a coordinate system defined by
$\theta$ together with $\{u_1, ..., u_n\}$, the $n$ uniform random numbers
used to construct a mock dataset.

In this alternative coordinate system, a proposed move
to change $\params$ while keeping
$\{u_i\}$ fixed results in a proposed new dataset that is similar to the
previous one, assuming the change to $\params$ is small. The standard procedure
of generating an entirely new mock dataset each step is now viewed as a
proposal where $\params$ is moved {\it and all of the $u_i$ coordinates are
re-generated from their iid uniform prior}.

\section*{Acknowledgements}
It is a pleasure to thank Ewan Cameron (Oxford), Dan
Foreman-Mackey (Washington), David Hogg (NYU), Daniela Huppenkothen (NYU),
Scott Sisson (UNSW), and Yanan Fan (NYU) for valuable discussions.

\begin{thebibliography}{99}
\bibitem[Caticha(2008)]{caticha} Caticha, A.\ 2008.\ Lectures 
on Probability, Entropy, and Statistical Physics.\ ArXiv e-prints 
arXiv:0808.0012. 

\end{thebibliography}

\end{document}


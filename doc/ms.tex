\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\newcommand{\params}{\boldsymbol{\theta}}	% The unknown parameters
\newcommand{\data}{\boldsymbol{D}}  % The data
\newcommand{\rands}{\boldsymbol{u}}
\newcommand{\dx}{d^N\mathbf{\params}} % Volume element in parameter space
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm

\title{Approximate Bayesian Computation with Nested Sampling}
\author{Brendon J. Brewer}

\begin{document}
\maketitle

\section{Introduction}


\section{Bayesian inference}
By the product rule of probability theory, the prior $p(\params)$ and the
conditional prior for the data $p(\data | \params)$ imply a joint prior
for $\params$ and $\data$:
\begin{equation}
\begin{array}{lr}
p(\params, \data) = p(\params)p(\data|\params)
\end{array}
\end{equation}

\section{Approximate Bayesian Computation}
Approximate Bayesian Computation (ABC), also known as likelihood-free inference,
is a set of Monte Carlo techniques for approximating a posterior distribution
without having to evaluate the likelihood function. Instead, the ability to
generate simulated data sets from the sampling distribution is required.
Since a sampling distribution must be specified, it is not the assumption of
a specific likelihood that is avoided (indeed, a sampling distribution and
a data set imply a particular likelihood function), rather that we cannot
evaluate it cheaply as a function of $\params$ and $\data$.

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lll}
\hline
Bayesian computation		&		Statistical mechanics		&		ABC\\
\hline
Parameter space	$\Theta$	&		Phase space	$\Omega$ or configuration space $\Gamma$ 			& (Parameter$\times$Data) space $\Theta \times \mathcal{D}$\\		
Marginal likelihood / evidence	&	Partition function at $T=1$	&\\


\end{tabular}
\caption{\it The relationship between standard Bayesian computation, statistical
mechanics, and ABC. In each case Monte Carlo methods are used to calculate
integrals over a space. In standard Bayesian computation, it is the parameter
space of a model that is usually of interest, whereas in ABC it is the space
of possible parameter values {\it and} data sets.
\label{tab:relation}}
\end{table}


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{figures/joint.pdf}
\caption{\it The joint prior for parameters $\params$ and data $\data$.
\label{fig:joint}}
\end{figure}

The main challenges associated with ABC are:
\begin{enumerate}
\item The choice of summary statistics
\item The choice of distance function
\item How to choose the value of $\epsilon$, or how to change it as a run
progresses
\item How to achieve good mixing.
\end{enumerate}
Challenges 1 and 2 are Bayesian in nature, i.e. they relate to the very
definition of the posterior distribution and are well defined independent of
the fact that we are going to use a Monte Carlo method to compute the
results. On the other hand, challenges 3 and 4 are about the computational implementation.

\section{Parameterising the data space}

It should be more efficient in general to parameterise the simulated dataset
by a set of rand calls, rather than in the coordinate system that naturally
describes the data. This allows MCMC moves to change $\params$ while keeping
$\rands$ fixed, resulting in a proposed new dataset that is similar to the
previous one, assuming the change to $\params$ is small.


\end{document}

